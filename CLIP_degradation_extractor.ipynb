{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class CLIP_Adapted_ImageEncoder(nn.Module):\n",
    "    def __init__(self, clip_model):\n",
    "        super().__init__()\n",
    "        self.clip_model = clip_model\n",
    "        \n",
    "        self.adapter = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            \n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Linear(512, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            \n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Linear(512, 512),\n",
    "            nn.LayerNorm(512)\n",
    "        )\n",
    "\n",
    "    def forward(self, image):\n",
    "        return self.adapter(self.clip_model.encode_image(image))\n",
    "    \n",
    "class CLIP_Adapted_TextEncoder(nn.Module):\n",
    "    def __init__(self, clip_model):\n",
    "        super().__init__()\n",
    "        self.clip_model = clip_model\n",
    "        self.adapter = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            \n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Linear(512, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            \n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Linear(512, 512),\n",
    "            nn.LayerNorm(512)\n",
    "        )\n",
    "\n",
    "    def forward(self, text):\n",
    "        return self.adapter(self.clip_model.encode_text(text))\n",
    "    \n",
    "class DA_adapter(nn.Module):\n",
    "    def __init__(self, clip_model):\n",
    "        super().__init__()\n",
    "        for param in clip_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.ad_imageEncoder = CLIP_Adapted_ImageEncoder(clip_model=clip_model)\n",
    "        self.ad_textEncoder = CLIP_Adapted_TextEncoder(clip_model=clip_model)\n",
    "        \n",
    "    def forward(self, image, text):\n",
    "        image = self.ad_imageEncoder(image)\n",
    "        text = self.ad_textEncoder(text)\n",
    "        return image, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "import model_resume\n",
    "import open_clip\n",
    "\n",
    "parser = ArgumentParser(description='LDUN')\n",
    "parser.add_argument('--about', type=str, default='5task')\n",
    "parser.add_argument('--start_epoch', type=int, default=0, help='epoch number of start training')\n",
    "parser.add_argument('--end_epoch', type=int, default=100, help='epoch number of end training')\n",
    "parser.add_argument('--learning_rate', type=float, default=1e-5, help='learning rate')\n",
    "parser.add_argument('--resume', type=bool, default=False, help='is resume')\n",
    "parser.add_argument('--group_num', type=int, default=1, help='group number for training')\n",
    "parser.add_argument('--gpu_list', type=str, default='1', help='gpu index')\n",
    "parser.add_argument('--checkpoints_dir', type=str, default='./checkpoints/2/', help='checkpoints dir')\n",
    "parser.add_argument('--log_dir', type=str, default='log', help='log directory')\n",
    "parser.add_argument('--ext', type=str, default='.png', help='training data directory')\n",
    "parser.add_argument('--is_aug', type=bool,default=False, help='is aug')\n",
    "parser.add_argument('--is_clip_tuning', type=bool,default=False, help='is finetuning clip')\n",
    "parser.add_argument('--patch_size', type=int, default=128, help='patchsize of input.')\n",
    "\n",
    "# Noise, Haze, Rain, Blurr, Lowlight\n",
    "NHRBL = [\"./datasets/denoising_datasets/15_train_paths.txt\",\n",
    " \"./datasets/denoising_datasets/25_train_paths.txt\",\n",
    " \"./datasets/denoising_datasets/50_train_paths.txt\",\n",
    " \"./datasets/dehazing_datasets/train_paths.txt\",\n",
    " \"./datasets/deraining_datasets/Rain100L/train_paths.txt\",\n",
    " \"./datasets/deblurring_datasets/GoPro/train_paths.txt\",\n",
    " \"./datasets/delowlight_datasets/LoL/train_paths.txt\"]\n",
    "\n",
    "# Noise, Haze, Rain\n",
    "NHR = [\"./datasets/denoising_datasets/15_train_paths.txt\",\n",
    " \"./datasets/denoising_datasets/25_train_paths.txt\",\n",
    " \"./datasets/denoising_datasets/50_train_paths.txt\",\n",
    " \"./datasets/dehazing_datasets/train_paths.txt\",\n",
    " \"./datasets/deraining_datasets/Rain100L/train_paths.txt\"]\n",
    "\n",
    "NHR_text = [\"image with noise\",\n",
    "            \"image with many noise\",\n",
    "            \"image with large amount of noise\",\n",
    "            \"image with haze\",\n",
    "            \"image with rain\"]\n",
    "\n",
    "NHRBL_text = [\"image with noise\",\n",
    "            \"image with many noise\",\n",
    "            \"image with large amount of noise\",\n",
    "            \"image with haze\",\n",
    "            \"image with rain\",\n",
    "            \"image with blur\",\n",
    "            \"image with low light\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(args=[])\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu_list\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess, _ = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')\n",
    "tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
    "\n",
    "model = DA_adapter(model)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=args.learning_rate, betas=(0.9, 0.999),eps=1e-8)\n",
    "\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(f\"{name} is learnable\")\n",
    "#     else:\n",
    "#         print(f\"{name} is frozen\")\n",
    "\n",
    "start_epoch = args.start_epoch\n",
    "new_lr = args.learning_rate\n",
    "######### Resume ###########\n",
    "if args.resume:\n",
    "    path_chk_rest = model_resume.get_last_path(args.checkpoints_dir, '_best.pth')\n",
    "    model_resume.load_checkpoint(model,path_chk_rest)\n",
    "    start_epoch = model_resume.load_start_epoch(path_chk_rest) + 1\n",
    "    model_resume.load_optim(optimizer, path_chk_rest)\n",
    "    \n",
    "    # for i in range(1, start_epoch):\n",
    "    #     scheduler.step()\n",
    "    # new_lr = scheduler.get_lr()[0]\n",
    "    \n",
    "######### Loss ###########\n",
    "criterion1 = nn.CrossEntropyLoss()\n",
    "criterion2 = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.clip_dataset import *\n",
    "trainset = TrainDataset_forIR(NHRBL, args)\n",
    "train_loader = DataLoader(trainset, batch_size=32, shuffle=True, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = tokenizer(NHRBL_text).to(device)\n",
    "for epoch in range(args.end_epoch):\n",
    "    epoch_loss = 0\n",
    "    epoch_start_time = time.time()\n",
    "    for i, data in enumerate(tqdm(train_loader), 0):\n",
    "        images = data[2].to(device) # [b, 3, 224, 224]\n",
    "        labels = data[3].to(device) # \n",
    "        \n",
    "        image_features, text_features = model(images, texts)\n",
    "\n",
    "        logits_per_image = image_features @ text_features.T\n",
    "        # selected_text_features = text_features[labels]\n",
    "        \n",
    "        # loss = criterion1(logits_per_image, labels) + criterion2(image_features, selected_text_features)\n",
    "        loss = criterion1(logits_per_image, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss +=loss.item()\n",
    "        if i % 2500 == 0:\n",
    "            text_probs = (100.0 * logits_per_image).softmax(dim=-1)\n",
    "            print(f\"Text Probs: {text_probs}\")\n",
    "            print(f\"Logits per Image: {logits_per_image}\")\n",
    "            print(f\"Lables: {labels}\")\n",
    "    epoch_end_time = time.time()\n",
    "    print(f\"Epoch {epoch+1}, Epoch Loss: {epoch_loss}, Time: {(epoch_end_time-epoch_start_time)}\")\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        learnable_params = {name: param for name, param in model.named_parameters() if param.requires_grad}\n",
    "        torch.save({'epoch': epoch, \n",
    "                    'learnable_params': learnable_params,\n",
    "                    'optimizer' : optimizer.state_dict()\n",
    "                    }, os.path.join(args.checkpoints_dir,f\"model_epoch_{epoch}.pth\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Image2Para",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
